{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L0MsS3bIpD_U",
        "X7YVc0dAxMMS",
        "fYuSh2fUxHl3",
        "16hTLb5_z8qH",
        "uyQjcCFU3G3T",
        "-xhDiv-F3G3U",
        "QmfxwzqJ3G3U",
        "AcS_3lly3G3V",
        "whPHSr5A4wG-",
        "wB5UJNcM4wG-",
        "fGdxEmlX5-2Y",
        "_sqEFpZF7ngR",
        "8yaVcbaeHmXE",
        "2tcq2lBG51sW"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "이 colab 노트는 AI End to End 과정을 실습하기 위해 만들어짐 (동대문 Sesac)"
      ],
      "metadata": {
        "id": "x2rt-MRnpKIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전 준비"
      ],
      "metadata": {
        "id": "L0MsS3bIpD_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요한 package 들을 설치"
      ],
      "metadata": {
        "id": "vzLhQtl8ozqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tree -y -qq"
      ],
      "metadata": {
        "id": "GBXuWN5Bo0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트에 사용할 Dataset 을 다운로드 후 train-data 폴더에 압축을 푼다"
      ],
      "metadata": {
        "id": "mMedJT_PoVJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbaccez6sexe"
      },
      "outputs": [],
      "source": [
        "# dataset download\n",
        "!wget -q https://github.com/max5982/ES/releases/download/simson/gray-400-50-simpson.zip\n",
        "!wget -q https://github.com/max5982/ES/releases/download/simson/gray-400-400-simpson.zip\n",
        "!wget -q https://github.com/max5982/ES/releases/download/simson/gray-test-simpson.zip\n",
        "!wget -q https://github.com/max5982/ES/releases/download/simson/gray-800-simpson.zip\n",
        "\n",
        "# 압축 풀기\n",
        "!unzip -q gray-400-50-simpson.zip -d train-data\n",
        "!unzip -q gray-400-400-simpson.zip -d train-400-data\n",
        "!unzip -q gray-800-simpson.zip -d train-800-data\n",
        "!unzip -q gray-test-simpson.zip -d test-data\n",
        "\n",
        "# 다운로드 받은 zip 파일 삭제\n",
        "!rm gray-400-50-simpson.zip\n",
        "!rm gray-400-400-simpson.zip\n",
        "!rm gray-test-simpson.zip\n",
        "!rm gray-800-simpson.zip\n",
        "\n",
        "# 잘 압축이 풀렸는지 tree 명령어로 Level 2 단계까지 확인\n",
        "!tree -L 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 dataset 의 갯수를 확인\n",
        "!find ./train-data/bart_simpson/ -type f | wc -l\n",
        "!find ./train-data/homer_simpson/ -type f | wc -l\n",
        "!find ./train-data/lisa_simpson/ -type f | wc -l\n",
        "!find ./train-data/marge_simpson/ -type f | wc -l"
      ],
      "metadata": {
        "id": "7iEYf0lAq-Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 첫번째 Practice - 2개의 hidden layer 를 가진 ANN model"
      ],
      "metadata": {
        "id": "X7YVc0dAxMMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의"
      ],
      "metadata": {
        "id": "FotiPk3aroSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ReLU 활성화 함수\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# ReLU의 미분 함수 (역전파용)\n",
        "def relu_deriv(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Softmax 활성화 함수 (출력층에 사용)\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "# 가중치 및 편향 초기화 함수\n",
        "def initialize_weights(input_size, hidden1, hidden2, output_size):\n",
        "    params = {\n",
        "        # 입력층 → 첫 번째 은닉층 가중치/편향\n",
        "        \"W1\": np.random.randn(input_size, hidden1) * 0.1,\n",
        "        \"b1\": np.zeros((1, hidden1)),\n",
        "        # 첫 번째 은닉층 → 두 번째 은닉층 가중치/편향\n",
        "        \"W2\": np.random.randn(hidden1, hidden2) * 0.1,\n",
        "        \"b2\": np.zeros((1, hidden2)),\n",
        "        # 두 번째 은닉층 → 출력층 가중치/편향\n",
        "        \"W3\": np.random.randn(hidden2, output_size) * 0.1,\n",
        "        \"b3\": np.zeros((1, output_size)),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "# 순전파(Forward Pass) 함수\n",
        "def forward_pass(x, params):\n",
        "    # 입력층 → 첫 번째 은닉층\n",
        "    z1 = x @ params[\"W1\"] + params[\"b1\"]\n",
        "    a1 = relu(z1)\n",
        "    # 첫 번째 은닉층 → 두 번째 은닉층\n",
        "    z2 = a1 @ params[\"W2\"] + params[\"b2\"]\n",
        "    a2 = relu(z2)\n",
        "    # 두 번째 은닉층 → 출력층\n",
        "    z3 = a2 @ params[\"W3\"] + params[\"b3\"]\n",
        "    out = softmax(z3)\n",
        "    # 순전파 중간 결과 저장 (역전파용)\n",
        "    cache = {\"x\": x, \"z1\": z1, \"a1\": a1, \"z2\": z2, \"a2\": a2, \"z3\": z3, \"out\": out}\n",
        "    return out, cache\n",
        "\n",
        "# 역전파(Backpropagation) 및 파라미터 업데이트 함수\n",
        "def backward_pass(params, cache, y_true, lr, sample_weights=None):\n",
        "    m = y_true.shape[0] # 데이터 샘플 개수\n",
        "    dz3 = cache[\"out\"] - y_true # 출력층 오차(softmax + cross-entropy)\n",
        "\n",
        "    # 샘플별 가중치 적용 (옵션)\n",
        "    if sample_weights is not None:\n",
        "        sample_weights = sample_weights.reshape(-1, 1)\n",
        "        dz3 *= sample_weights\n",
        "\n",
        "    # 두 번째 은닉층 → 출력층 가중치/편향의 기울기\n",
        "    dW3 = cache[\"a2\"].T @ dz3 / m\n",
        "    db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
        "\n",
        "    # 출력층 → 두 번째 은닉층 오차 전파\n",
        "    da2 = dz3 @ params[\"W3\"].T\n",
        "    dz2 = da2 * relu_deriv(cache[\"z2\"])\n",
        "    dW2 = cache[\"a1\"].T @ dz2 / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # 두 번째 은닉층 → 첫 번째 은닉층 오차 전파\n",
        "    da1 = dz2 @ params[\"W2\"].T\n",
        "    dz1 = da1 * relu_deriv(cache[\"z1\"])\n",
        "    dW1 = cache[\"x\"].T @ dz1 / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # 가중치 및 편향 업데이트 (경사하강법)\n",
        "    params[\"W3\"] -= lr * dW3\n",
        "    params[\"b3\"] -= lr * db3\n",
        "    params[\"W2\"] -= lr * dW2\n",
        "    params[\"b2\"] -= lr * db2\n",
        "    params[\"W1\"] -= lr * dW1\n",
        "    params[\"b1\"] -= lr * db1"
      ],
      "metadata": {
        "id": "l2VAfR97xQjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training"
      ],
      "metadata": {
        "id": "35VkEOfkul8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img = Image.open(img_path).convert(\"L\").resize((28, 28))\n",
        "    return np.array(img).flatten().astype(np.float32) / 255.0\n",
        "\n",
        "def one_hot_encode(label_index, num_classes):\n",
        "    one_hot = np.zeros(num_classes)\n",
        "    one_hot[label_index] = 1.0\n",
        "    return one_hot\n",
        "\n",
        "def load_dataset(base_path):\n",
        "    X, y = [], []\n",
        "    label_map = {}\n",
        "    for idx, label_name in enumerate(sorted(os.listdir(base_path))):\n",
        "        label_map[label_name] = idx\n",
        "        folder = os.path.join(base_path, label_name)\n",
        "        for file in os.listdir(folder):\n",
        "            img_path = os.path.join(folder, file)\n",
        "            x = preprocess_image(img_path)\n",
        "            X.append(x)\n",
        "            y.append(one_hot_encode(idx, len(os.listdir(base_path))))\n",
        "    return np.array(X), np.array(y), label_map\n",
        "\n",
        "def train(X_train, y_train, params, lr, epochs):\n",
        "    losses, accuracies = [], []\n",
        "    for epoch in range(epochs):\n",
        "        out, cache = forward_pass(X_train, params)\n",
        "        loss = -np.sum(y_train * np.log(out + 1e-9)) / y_train.shape[0]\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        labels = np.argmax(y_train, axis=1)\n",
        "        acc = np.mean(preds == labels)\n",
        "\n",
        "        losses.append(loss)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "        backward_pass(params, cache, y_train, lr)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, epochs + 1), losses, label='Loss', color='orange')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, epochs + 1), accuracies, label='Accuracy', color='orange')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "KgGZIB6K0KNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, y_train, label_map = load_dataset(\"train-data\")\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 128, 64, output_size)\n",
        "\n",
        "# lr - learning rate\n",
        "# epochs - 몇번 훈련할지 설정\n",
        "params = train(X_train, y_train, params, lr=0.01, epochs=50)\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "DgiC9zfOsBxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인"
      ],
      "metadata": {
        "id": "21ObbdvDvY2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "RNF2xi2J21N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "azEXMrmh27gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 두번째 Practice - Training 데이터셋의 분할과 Early Stop"
      ],
      "metadata": {
        "id": "fYuSh2fUxHl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training 하는 부분 재정의\n",
        " - patience: 몇번이나 성능개선 없는지 count MAX\n",
        " - delta: 유의미한 성능 개선의 범위\n",
        " - test_size: Training : Validation ratio"
      ],
      "metadata": {
        "id": "nqk8NnN2x5Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train(X_train, y_train, X_val, y_val, params, lr, epochs, patience=5, delta=0.001):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_loss = np.inf\n",
        "    best_params = {k: v.copy() for k, v in params.items()}\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        out_train, cache_train = forward_pass(X_train, params)\n",
        "        train_loss = -np.sum(y_train * np.log(out_train + 1e-9)) / y_train.shape[0]\n",
        "        train_preds = np.argmax(out_train, axis=1)\n",
        "        train_acc = np.mean(train_preds == np.argmax(y_train, axis=1))\n",
        "\n",
        "        backward_pass(params, cache_train, y_train, lr)\n",
        "\n",
        "        out_val, _ = forward_pass(X_val, params)\n",
        "        val_loss = -np.sum(y_val * np.log(out_val + 1e-9)) / y_val.shape[0]\n",
        "        val_preds = np.argmax(out_val, axis=1)\n",
        "        val_acc = np.mean(val_preds == np.argmax(y_val, axis=1))\n",
        "\n",
        "        if val_loss < best_loss - delta:\n",
        "            best_loss = val_loss\n",
        "            best_params = {k: v.copy() for k, v in params.items()}\n",
        "            no_improve = 0\n",
        "            print(f\"★ Validation improved to {val_loss:.4f}\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
        "              f\"No improve: {no_improve}/{patience}\")\n",
        "\n",
        "        # Early Stop\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}!\")\n",
        "            params = {k: v.copy() for k, v in best_params.items()}\n",
        "            break\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', marker='o', color='orange')\n",
        "    plt.plot(val_losses, label='Val Loss', marker='x', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training vs Validation Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Train Acc', marker='o', color='orange')\n",
        "    plt.plot(val_accuracies, label='Val Acc', marker='x', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "zbB64ls87mx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training"
      ],
      "metadata": {
        "id": "u09QBnv0y8fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_full, y_full, label_map = load_dataset(\"train-data\")\n",
        "y_labels = np.argmax(y_full, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    # Training dataset split\n",
        "    test_size=0.2, # 8:2 -> train:val ratio\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 128, 64, output_size)\n",
        "\n",
        "params = train(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    params, lr=0.01, epochs=100,\n",
        "    patience=5, delta=0.0015\n",
        ")\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "dl-NePVoy-9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인"
      ],
      "metadata": {
        "id": "whYj99unzOGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "t5ZNeIHizRt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "EIcwLRC6zbhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3KKjbBxdz6ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 세번째 Practice - Training 데이터셋의 불균형을 해결하기 위한 가중치 조절"
      ],
      "metadata": {
        "id": "16hTLb5_z8qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training 하는 부분 재정의\n"
      ],
      "metadata": {
        "id": "x_kNlv3Az8qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "def train(X_train, y_train, X_val, y_val, params, lr, epochs, patience=5, delta=0.001, class_weights=None):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_loss = np.inf\n",
        "    best_params = {k: v.copy() for k, v in params.items()}\n",
        "    no_improve = 0\n",
        "\n",
        "    if class_weights is not None:\n",
        "        class_weights = np.array(class_weights)\n",
        "        sample_weights = class_weights[np.argmax(y_train, axis=1)]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        out_train, cache_train = forward_pass(X_train, params)\n",
        "\n",
        "        if class_weights is not None:\n",
        "            loss_per_sample = -np.sum(y_train * np.log(out_train + 1e-9), axis=1)\n",
        "            train_loss = np.sum(loss_per_sample * sample_weights) / y_train.shape[0]\n",
        "        else:\n",
        "            train_loss = -np.sum(y_train * np.log(out_train + 1e-9)) / y_train.shape[0]\n",
        "\n",
        "        train_preds = np.argmax(out_train, axis=1)\n",
        "        train_acc = np.mean(train_preds == np.argmax(y_train, axis=1))\n",
        "\n",
        "        # dataset 에 가중치를 부여하는 부분\n",
        "        if class_weights is not None:\n",
        "            backward_pass(params, cache_train, y_train, lr, sample_weights)\n",
        "        else:\n",
        "            backward_pass(params, cache_train, y_train, lr)\n",
        "\n",
        "        out_val, _ = forward_pass(X_val, params)\n",
        "        val_loss = -np.sum(y_val * np.log(out_val + 1e-9)) / y_val.shape[0]\n",
        "        val_preds = np.argmax(out_val, axis=1)\n",
        "        val_acc = np.mean(val_preds == np.argmax(y_val, axis=1))\n",
        "\n",
        "        if val_loss < best_loss - delta:\n",
        "            best_loss = val_loss\n",
        "            best_params = {k: v.copy() for k, v in params.items()}\n",
        "            no_improve = 0\n",
        "            print(f\"★ Validation improved to {val_loss:.4f}\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
        "              f\"No improve: {no_improve}/{patience}\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}!\")\n",
        "            params = {k: v.copy() for k, v in best_params.items()}\n",
        "            break\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', marker='o', color='orange')\n",
        "    plt.plot(val_losses, label='Val Loss', marker='x', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training vs Validation Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Train Acc', marker='o', color='orange')\n",
        "    plt.plot(val_accuracies, label='Val Acc', marker='x', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "TuFZV_7lz8qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training"
      ],
      "metadata": {
        "id": "t0eRHxyoz8qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_full, y_full, label_map = load_dataset(\"train-data\")\n",
        "y_labels = np.argmax(y_full, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    # Training dataset split\n",
        "    test_size=0.2, # 8:2 -> train:val ratio\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# dataset 의 불균형을 해소하기 위한 weight 값을 통한 가중치 부여\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights = weights.astype(np.float32)\n",
        "print(f\"\\nClass Weights: {class_weights}\\n\")\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 128, 64, output_size)\n",
        "\n",
        "params = train(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    params, lr=0.01, epochs=100,\n",
        "    patience=5, delta=0.0015,\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "LKHmPPYTz8qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인"
      ],
      "metadata": {
        "id": "0mGP3n4Ez8qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "52rStqXwz8qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "KVN3kMSEz8qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 네번째 Practice - balanced dataset 을 충분한 양으로 늘려줬을 경우"
      ],
      "metadata": {
        "id": "uyQjcCFU3G3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 400 개의 balanced dataset\n"
      ],
      "metadata": {
        "id": "-xhDiv-F3G3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 dataset 의 갯수를 확인\n",
        "!find ./train-400-data/bart_simpson/ -type f | wc -l\n",
        "!find ./train-400-data/homer_simpson/ -type f | wc -l\n",
        "!find ./train-400-data/lisa_simpson/ -type f | wc -l\n",
        "!find ./train-400-data/marge_simpson/ -type f | wc -l"
      ],
      "metadata": {
        "id": "BfbaomDy3G3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training - 400/400/400/400"
      ],
      "metadata": {
        "id": "QmfxwzqJ3G3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_full, y_full, label_map = load_dataset(\"train-400-data\")\n",
        "y_labels = np.argmax(y_full, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    # Training dataset split\n",
        "    test_size=0.2, # 8:2 -> train:val ratio\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# dataset 의 불균형을 해소하기 위한 weight 값을 통한 가중치 부여\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights = weights.astype(np.float32)\n",
        "print(f\"\\nClass Weights: {class_weights}\\n\")\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 128, 64, output_size)\n",
        "\n",
        "params = train(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    params, lr=0.01, epochs=100,\n",
        "    patience=5, delta=0.0015,\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "HbztjxLQ3G3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인 - 400/400/400/400"
      ],
      "metadata": {
        "id": "AcS_3lly3G3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "IJYu8R7f3G3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "69r_UT6r3G3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model.pkl')"
      ],
      "metadata": {
        "id": "Dulqe6-M3G3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJcploph3G3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 800 개의 balanced dataset\n"
      ],
      "metadata": {
        "id": "L-4DyuMh4H_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 dataset 의 갯수를 확인\n",
        "!find ./train-800-data/bart_simpson/ -type f | wc -l\n",
        "!find ./train-800-data/homer_simpson/ -type f | wc -l\n",
        "!find ./train-800-data/lisa_simpson/ -type f | wc -l\n",
        "!find ./train-800-data/marge_simpson/ -type f | wc -l"
      ],
      "metadata": {
        "id": "4uZqOzkX4H_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training - 800/800/800/800"
      ],
      "metadata": {
        "id": "e8SaxW1x4KLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_full, y_full, label_map = load_dataset(\"train-800-data\")\n",
        "y_labels = np.argmax(y_full, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    # Training dataset split\n",
        "    test_size=0.2, # 8:2 -> train:val ratio\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# dataset 의 불균형을 해소하기 위한 weight 값을 통한 가중치 부여\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights = weights.astype(np.float32)\n",
        "print(f\"\\nClass Weights: {class_weights}\\n\")\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 128, 64, output_size)\n",
        "\n",
        "params = train(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    params, lr=0.01, epochs=100,\n",
        "    patience=5, delta=0.0015,\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "tpUd-4WA4KLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인 - 800/800/800/800"
      ],
      "metadata": {
        "id": "EkLwmLAi4Muq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "_cbe6hmL4Muq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "afxQFqVq4Mur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다섯번째 Practice - Batch size 적용"
      ],
      "metadata": {
        "id": "whPHSr5A4wG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training 하는 부분 재정의\n"
      ],
      "metadata": {
        "id": "wB5UJNcM4wG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, y_train, X_val, y_val, params, lr, epochs, patience=5, delta=0.001, class_weights=None, batch_size=64):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_loss = np.inf\n",
        "    best_params = {k: v.copy() for k, v in params.items()}\n",
        "    no_improve = 0\n",
        "\n",
        "    if class_weights is not None:\n",
        "        sample_weights = np.array([class_weights[cls] for cls in np.argmax(y_train, axis=1)])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(X_train.shape[0])\n",
        "        X_shuffled = X_train[indices]\n",
        "        y_shuffled = y_train[indices]\n",
        "        if class_weights is not None:\n",
        "            sw_shuffled = sample_weights[indices]\n",
        "        else:\n",
        "            sw_shuffled = None\n",
        "\n",
        "        # 배치로 나누기\n",
        "        n_batches = X_train.shape[0] // batch_size\n",
        "        if X_train.shape[0] % batch_size != 0:\n",
        "            n_batches += 1\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_acc = 0.0\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            start = i * batch_size\n",
        "            end = min((i+1)*batch_size, X_train.shape[0])\n",
        "            X_batch = X_shuffled[start:end]\n",
        "            y_batch = y_shuffled[start:end]\n",
        "            if sw_shuffled is not None:\n",
        "                sw_batch = sw_shuffled[start:end]\n",
        "            else:\n",
        "                sw_batch = None\n",
        "\n",
        "            # Forward pass\n",
        "            out_batch, cache_batch = forward_pass(X_batch, params)\n",
        "\n",
        "            if class_weights is not None:\n",
        "                loss_per_sample = -np.sum(y_batch * np.log(out_batch + 1e-9), axis=1)\n",
        "                batch_loss = np.sum(loss_per_sample * sw_batch) / X_batch.shape[0]\n",
        "            else:\n",
        "                batch_loss = -np.sum(y_batch * np.log(out_batch + 1e-9)) / X_batch.shape[0]\n",
        "\n",
        "            batch_preds = np.argmax(out_batch, axis=1)\n",
        "            batch_acc = np.mean(batch_preds == np.argmax(y_batch, axis=1))\n",
        "\n",
        "            if class_weights is not None:\n",
        "                backward_pass(params, cache_batch, y_batch, lr, sw_batch)\n",
        "            else:\n",
        "                backward_pass(params, cache_batch, y_batch, lr)\n",
        "\n",
        "            epoch_train_loss += batch_loss * X_batch.shape[0]\n",
        "            epoch_train_acc += batch_acc * X_batch.shape[0]\n",
        "\n",
        "        train_loss = epoch_train_loss / X_train.shape[0]\n",
        "        train_acc = epoch_train_acc / X_train.shape[0]\n",
        "\n",
        "        out_val, _ = forward_pass(X_val, params)\n",
        "        val_loss = -np.sum(y_val * np.log(out_val + 1e-9)) / y_val.shape[0]\n",
        "        val_preds = np.argmax(out_val, axis=1)\n",
        "        val_acc = np.mean(val_preds == np.argmax(y_val, axis=1))\n",
        "\n",
        "        if val_loss < best_loss - delta:\n",
        "            best_loss = val_loss\n",
        "            best_params = {k: v.copy() for k, v in params.items()}\n",
        "            no_improve = 0\n",
        "            print(f\"★ Validation improved to {val_loss:.4f}\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
        "              f\"No improve: {no_improve}/{patience}\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}!\")\n",
        "            params = {k: v.copy() for k, v in best_params.items()}\n",
        "            break\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', marker='o', color='orange')\n",
        "    plt.plot(val_losses, label='Val Loss', marker='x', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training vs Validation Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Train Acc', marker='o', color='orange')\n",
        "    plt.plot(val_accuracies, label='Val Acc', marker='x', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "wDKLfIeL4wG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training"
      ],
      "metadata": {
        "id": "HjeDageo4wG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_full, y_full, label_map = load_dataset(\"train-800-data\")\n",
        "y_labels = np.argmax(y_full, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    # Training dataset split\n",
        "    test_size=0.2, # 8:2 -> train:val ratio\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# dataset 의 불균형을 해소하기 위한 weight 값을 통한 가중치 부여\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights = weights.astype(np.float32)\n",
        "print(f\"\\nClass Weights: {class_weights}\\n\")\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 128, 64, output_size)\n",
        "\n",
        "params = train(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    params, lr=0.01, epochs=100,\n",
        "    patience=5, delta=0.0015,\n",
        "    class_weights=class_weights,\n",
        "    # Batch Size 를 정의/적용\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "qC5DNdfO4wG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인"
      ],
      "metadata": {
        "id": "zv5kujRj4wG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "tNnOtof54wG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "dl2TDToN4wG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 여섯번째 Practice - hidden layer 추가"
      ],
      "metadata": {
        "id": "fGdxEmlX5-2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 재정의"
      ],
      "metadata": {
        "id": "Z0ZpuTJa6jx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(input_size, hidden1, hidden2, hidden3, output_size):\n",
        "    params = {\n",
        "        \"W1\": np.random.randn(input_size, hidden1) * 0.1,\n",
        "        \"b1\": np.zeros((1, hidden1)),\n",
        "        \"W2\": np.random.randn(hidden1, hidden2) * 0.1,\n",
        "        \"b2\": np.zeros((1, hidden2)),\n",
        "        \"W3\": np.random.randn(hidden2, hidden3) * 0.1,\n",
        "        \"b3\": np.zeros((1, hidden3)),\n",
        "        \"W4\": np.random.randn(hidden3, output_size) * 0.1,\n",
        "        \"b4\": np.zeros((1, output_size)),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def forward_pass(x, params):\n",
        "    z1 = x @ params[\"W1\"] + params[\"b1\"]\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1 @ params[\"W2\"] + params[\"b2\"]\n",
        "    a2 = relu(z2)\n",
        "    z3 = a2 @ params[\"W3\"] + params[\"b3\"]\n",
        "    a3 = relu(z3)\n",
        "    z4 = a3 @ params[\"W4\"] + params[\"b4\"]\n",
        "    out = softmax(z4)\n",
        "    cache = {\"x\":x, \"z1\":z1, \"a1\":a1, \"z2\":z2, \"a2\":a2, \"z3\":z3, \"a3\":a3, \"z4\":z4, \"out\":out}\n",
        "    return out, cache\n",
        "\n",
        "def backward_pass(params, cache, y_true, lr, sample_weights=None):\n",
        "    m = y_true.shape[0]\n",
        "    dz4 = cache[\"out\"] - y_true\n",
        "\n",
        "    if sample_weights is not None:\n",
        "       sample_weights = sample_weights.reshape(-1, 1)\n",
        "       dz4 *= sample_weights\n",
        "\n",
        "    dW4 = (cache[\"a3\"].T @ dz4) / m\n",
        "    db4 = np.sum(dz4, axis=0, keepdims=True) / m\n",
        "\n",
        "    da3 = dz4 @ params[\"W4\"].T\n",
        "    dz3 = da3 * relu_deriv(cache[\"z3\"])\n",
        "    dW3 = (cache[\"a2\"].T @ dz3) / m\n",
        "    db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
        "\n",
        "    da2 = dz3 @ params[\"W3\"].T\n",
        "    dz2 = da2 * relu_deriv(cache[\"z2\"])\n",
        "    dW2 = (cache[\"a1\"].T @ dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    da1 = dz2 @ params[\"W2\"].T\n",
        "    dz1 = da1 * relu_deriv(cache[\"z1\"])\n",
        "    dW1 = (cache[\"x\"].T @ dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    params[\"W4\"] -= lr * dW4\n",
        "    params[\"b4\"] -= lr * db4\n",
        "    params[\"W3\"] -= lr * dW3\n",
        "    params[\"b3\"] -= lr * db3\n",
        "    params[\"W2\"] -= lr * dW2\n",
        "    params[\"b2\"] -= lr * db2\n",
        "    params[\"W1\"] -= lr * dW1\n",
        "    params[\"b1\"] -= lr * db1"
      ],
      "metadata": {
        "id": "chT8ZtmN6luz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training"
      ],
      "metadata": {
        "id": "yXHq6iRs5-2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_full, y_full, label_map = load_dataset(\"train-800-data\")\n",
        "y_labels = np.argmax(y_full, axis=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    # Training dataset split\n",
        "    test_size=0.2, # 8:2 -> train:val ratio\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# dataset 의 불균형을 해소하기 위한 weight 값을 통한 가중치 부여\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights = weights.astype(np.float32)\n",
        "print(f\"\\nClass Weights: {class_weights}\\n\")\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "params = initialize_weights(input_size, 512, 256, 128, output_size)\n",
        "\n",
        "params = train(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    params, lr=0.01, epochs=100,\n",
        "    patience=5, delta=0.0015,\n",
        "    class_weights=class_weights,\n",
        "    # Batch Size 를 정의/적용\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((params, label_map), f)"
      ],
      "metadata": {
        "id": "rrX5xzWQ5-2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인"
      ],
      "metadata": {
        "id": "mma_lTnR5-2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data = []\n",
        "    filenames = []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"L\").resize((28, 28))\n",
        "            arr = np.array(img).reshape(-1) / 255.0\n",
        "            data.append(arr)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return np.array(data), filenames\n",
        "\n",
        "def inference():\n",
        "    with open(\"model.pkl\", \"rb\") as f:\n",
        "        params, label_map = pickle.load(f)\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        out, _ = forward_pass(X, params)\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "eU_IRTB25-2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_test, y_test, label_map = load_dataset(\"test-data\")\n",
        "with open(\"model.pkl\", \"rb\") as f:\n",
        "    params, label_map = pickle.load(f)\n",
        "\n",
        "out, _ = forward_pass(X_test, params)\n",
        "preds = np.argmax(out, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, preds, target_names=label_map.keys()))"
      ],
      "metadata": {
        "id": "2Mq4wnbD5-2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 일곱번째 Practice - Pytorch 사용하기"
      ],
      "metadata": {
        "id": "_sqEFpZF7ngR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의"
      ],
      "metadata": {
        "id": "h_oTNn2lAOxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.out = nn.Linear(hidden_sizes[2], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.out(x)\n",
        "\n",
        "def preprocess_image_tensor(img_path):\n",
        "    img = Image.open(img_path).convert(\"L\").resize((28, 28))\n",
        "    img = torch.tensor(list(img.getdata()), dtype=torch.float32).view(1, 28, 28)\n",
        "    return img.view(-1) / 255.0\n",
        "\n",
        "def one_hot_encode_tensor(label_index, num_classes):\n",
        "    return torch.nn.functional.one_hot(torch.tensor(label_index), num_classes).float()\n",
        "\n",
        "def load_dataset_tensor(base_path):\n",
        "    X, y = [], []\n",
        "    label_map = {}\n",
        "    class_names = sorted(os.listdir(base_path))\n",
        "    for idx, label_name in enumerate(class_names):\n",
        "        label_map[label_name] = idx\n",
        "        folder = os.path.join(base_path, label_name)\n",
        "        for file in os.listdir(folder):\n",
        "            img_path = os.path.join(folder, file)\n",
        "            try:\n",
        "                x = preprocess_image_tensor(img_path)\n",
        "                X.append(x)\n",
        "                y.append(one_hot_encode_tensor(idx, len(class_names)))\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {img_path}: {e}\")\n",
        "    return torch.stack(X), torch.stack(y), label_map"
      ],
      "metadata": {
        "id": "93_sXDJyAQ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 Training"
      ],
      "metadata": {
        "id": "7CaCykNaArJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(X_train, y_train, X_val, y_val, label_map, epochs=100, lr=0.001, patience=5, delta=0.0015, batch_size=8):\n",
        "    print(f\"** cuda? - {torch.cuda.is_available()} **\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = y_train.shape[1]\n",
        "    hidden_sizes = [512, 256, 128]\n",
        "\n",
        "    model = DNN(input_size, hidden_sizes, output_size).to(device)\n",
        "\n",
        "    labels = torch.argmax(y_train, dim=1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = labels.to(device)\n",
        "    X_val = X_val.to(device)\n",
        "    y_val = torch.argmax(y_val, dim=1).to(device)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    best_model = None\n",
        "    best_loss = float(\"inf\")\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        permutation = torch.randperm(X_train.size(0))\n",
        "        total_loss, correct = 0.0, 0\n",
        "\n",
        "        for i in range(0, X_train.size(0), batch_size):\n",
        "            idx = permutation[i:i+batch_size]\n",
        "            batch_x = X_train[idx]\n",
        "            batch_y = y_train[idx]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
        "\n",
        "        train_loss = total_loss / X_train.size(0)\n",
        "        train_acc = correct / X_train.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val).item()\n",
        "            val_preds = val_outputs.argmax(dim=1)\n",
        "            val_acc = (val_preds == y_val).float().mean().item()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
        "              f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_loss < best_loss - delta:\n",
        "            best_loss = val_loss\n",
        "            best_model = model.state_dict()\n",
        "            no_improve = 0\n",
        "            print(\"★ Validation improved\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    torch.save({'model_state_dict': best_model, 'label_map': label_map}, 'model_tensor.pth')\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label=\"Train Acc\")\n",
        "    plt.plot(val_accuracies, label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(\"training_metrics_tensor.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "X, y, label_map = load_dataset_tensor(\"train-800-data\")\n",
        "y_np = torch.argmax(y, dim=1).numpy()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y_np)\n",
        "train_model(X_train, y_train, X_val, y_val, label_map)"
      ],
      "metadata": {
        "id": "s-XkIp8zAvaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 결과 확인"
      ],
      "metadata": {
        "id": "rF1OwW4bGUVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_in_folder(folder):\n",
        "    data, filenames = [], []\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            data.append(preprocess_image_tensor(path))\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "    return torch.stack(data), filenames if data else (torch.empty(0), filenames)\n",
        "\n",
        "def inference():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    checkpoint = torch.load(\"model_tensor.pth\", map_location=device)\n",
        "    label_map = checkpoint[\"label_map\"]\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    input_size = 28 * 28\n",
        "    output_size = len(label_map)\n",
        "    hidden_sizes = [512, 256, 128]\n",
        "\n",
        "    model = DNN(input_size, hidden_sizes, output_size).to(device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct, total_samples = 0, 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if X.numel() == 0:\n",
        "            print(f\"{class_name}: No images found.\")\n",
        "            continue\n",
        "\n",
        "        X = X.to(device)\n",
        "        outputs = model(X)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu()\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        correct = 0\n",
        "        print(f\"\\nClass: {class_name}\\n\" + \"-\"*40)\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred.item()]\n",
        "            is_correct = pred.item() == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        acc = correct / len(preds) * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{len(preds)})\")\n",
        "        total_correct += correct\n",
        "        total_samples += len(preds)\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "AmwBWGfZGR35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 여덜번째 Practice - Transfer Learning"
      ],
      "metadata": {
        "id": "8yaVcbaeHmXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resnet50"
      ],
      "metadata": {
        "id": "qqB8kwOHICG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def get_dataloaders(data_dir, batch_size=32, val_ratio=0.2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "    class_names = dataset.classes\n",
        "\n",
        "    val_size = int(len(dataset) * val_ratio)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, class_names\n",
        "\n",
        "def get_resnet50(num_classes):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "    #for param in model.parameters():\n",
        "    #    param.requires_grad = False\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    #for param in model.fc.parameters():\n",
        "    #    param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "def get_mobilenetv2(num_classes):\n",
        "    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_model(data_dir=\"train-800-data\", epochs=10, lr=1e-4, batch_size=32):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"device {}\".format(device))\n",
        "    train_loader, val_loader, class_names = get_dataloaders(data_dir, batch_size)\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    model = get_resnet50(num_classes).to(device)\n",
        "    #model = get_mobilenetv2(num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_preds, train_labels = [], []\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_resnet50.pth\")\n",
        "            #torch.save(model.state_dict(), \"best_mobilenetv2.pth\")\n",
        "            with open(\"class_names.pkl\", \"wb\") as f:\n",
        "                pickle.dump(class_names, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "25DhysXCIFMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련결과 확인"
      ],
      "metadata": {
        "id": "oc43UOCVJEUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pickle\n",
        "from torchvision import models\n",
        "\n",
        "def load_images_in_folder(folder):\n",
        "    images = []\n",
        "    filenames = []\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        path = os.path.join(folder, file)\n",
        "        try:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img_tensor = transform(img)\n",
        "            images.append(img_tensor)\n",
        "            filenames.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    if images:\n",
        "        images = torch.stack(images)  # Shape: (N, 3, 224, 224)\n",
        "    else:\n",
        "        images = torch.empty((0, 3, 224, 224))\n",
        "\n",
        "    return images, filenames\n",
        "\n",
        "def inference():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load class labels\n",
        "    with open(\"class_names.pkl\", \"rb\") as f:\n",
        "        class_names = pickle.load(f)\n",
        "    label_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Load model\n",
        "    model = models.resnet50(weights=None)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(\"best_resnet50.pth\", map_location=device))\n",
        "    #model.load_state_dict(torch.load(\"best_mobilenetv2.pth\", map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_root = \"test-data\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Detailed Inference Results\\n==========================\")\n",
        "\n",
        "    for class_name in sorted(os.listdir(test_root)):\n",
        "        folder = os.path.join(test_root, class_name)\n",
        "        if not os.path.isdir(folder): continue\n",
        "\n",
        "        X, filenames = load_images_in_folder(folder)\n",
        "        if len(X) == 0:\n",
        "            print(f\"\\n{class_name}: No images found.\\n\")\n",
        "            continue\n",
        "\n",
        "        X = X.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        true_label = label_map[class_name]\n",
        "\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(preds):\n",
        "            pred_label = id_to_label[pred]\n",
        "            is_correct = pred == true_label\n",
        "            mark = \"✔️\" if is_correct else \"❌\"\n",
        "            print(f\"{filenames[i]:25} → Predicted: {pred_label:15} {mark}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "\n",
        "        total = len(preds)\n",
        "        acc = correct / total * 100\n",
        "        print(f\"\\n{class_name} Accuracy: {acc:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        total_correct += correct\n",
        "        total_samples += total\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(f\"Overall Accuracy : {total_correct / total_samples * 100:.2f}% ({total_correct}/{total_samples})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "CgYmEKdkJHuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 마무리"
      ],
      "metadata": {
        "id": "2tcq2lBG51sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "p5XnX2IxMUA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련해서 나온 pre-trained model 을 로컬로 다운로드 하기\n",
        "from google.colab import files\n",
        "\n",
        "files.download('best_resnet50.pth')\n",
        "#files.download('best_mobilenetv2.pth')"
      ],
      "metadata": {
        "id": "tR-pZjOt4wG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://netron.app/ 에서 best_resnet50.pth 를 열어서 network 구조를 살펴보자"
      ],
      "metadata": {
        "id": "hYePS7dFMmtd"
      }
    }
  ]
}